{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_lg', disable=[\"parser\", \"tagger\", \"ner\"])\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import scipy\n",
    "import collections\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "from scipy import sparse\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "EPS = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from . import FUN_FACT_CSV, REQUIRED_COLUMNS\n",
    "TIL_TITLE_CSV = 'data/til_title.csv'\n",
    "REQUIRED_COLUMNS= [\"title\", \"score\", \"permalink\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Arshi/anaconda/envs/myenv/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3049: DtypeWarning: Columns (0,1,2,3,5,8,9,13,14,15,16,19,21,22,24,27,28,33,34,35,41,46,58,60,62,63,64,65,68,74,79,85,92,121) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing tf-idf matrix\n",
      "Loading spacy\n",
      "Computing weighted embeddings\n",
      "Done loading 324996 rows\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading data csv\")\n",
    "#fun_fact_title_data = pd.read_csv(FUN_FACT_TITLE_CSV).dropna(subset=REQUIRED_COLUMNS)\n",
    "til_title_data = pd.read_csv(TIL_TITLE_CSV).dropna(subset=REQUIRED_COLUMNS)\n",
    "#ysk_title_data = pd.read_csv(YSK_TITLE_CSV).dropna(subset=REQUIRED_COLUMNS)\n",
    "\n",
    "title_data = pd.concat([\n",
    "    #fun_fact_title_data,\n",
    "    til_title_data,\n",
    "    #ysk_title_data,\n",
    "], join='inner').reset_index(drop=True)\n",
    "\n",
    "print(\"Computing tf-idf matrix\")\n",
    "vectorizer = TfidfVectorizer(stop_words='english', dtype=np.float32)\n",
    "tfidf_matrix = vectorizer.fit_transform(title_data[\"title\"])\n",
    "\n",
    "print(\"Loading spacy\")\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "print(\"Computing weighted embeddings\")\n",
    "features = vectorizer.get_feature_names()\n",
    "f_vectors = np.array([nlp.vocab[f].vector for f in features])\n",
    "weighted_embeddings = tfidf_matrix.dot(f_vectors)\n",
    "assert weighted_embeddings.shape == (len(title_data.index), 300)\n",
    "n_weighted_embeddings = weighted_embeddings / (np.linalg.norm(weighted_embeddings, axis=1)[:, np.newaxis] + EPS)\n",
    "\n",
    "#print(\"Compressing pandas dataframe into index\")\n",
    "#self.index = list(title_data.itertuples())\n",
    "\n",
    "print(\"Done loading {} rows\".format(len(title_data.index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query, method = 'similarity', top=10, rocchio = False):\n",
    "    if rocchio:\n",
    "        query_weighted = query\n",
    "    else:\n",
    "        query_tfidf = vectorizer.transform([query])\n",
    "        if query_tfidf.count_nonzero() > 0:\n",
    "            query_weighted = query_tfidf.dot(f_vectors).flatten()\n",
    "        # average word embeddings if query words don't exist in our corpus (tfidf matrix)\n",
    "        else:\n",
    "            tokens = vectorizer.build_analyzer()(query)\n",
    "            # query was all stopwords, so we'll have to manually tokenize\n",
    "            if not tokens:\n",
    "                tokens = query.lower().split()\n",
    "            query_weighted = np.average([nlp.vocab[t].vector for t in tokens], axis=0).flatten()\n",
    "#     print('query_weighted: ', repr(query_weighted))\n",
    "\n",
    "    # if we have no embeddings for the given query, we're out of luck\n",
    "    if np.count_nonzero(query_weighted) == 0:\n",
    "        return []\n",
    "\n",
    "    n_query_weighted = query_weighted / (np.linalg.norm(query_weighted) + EPS)\n",
    "    rankings = n_weighted_embeddings.dot(n_query_weighted)\n",
    "    rankings_index = np.argsort(-rankings)\n",
    "    ranked_df = title_data.loc[rankings_index]\n",
    "    ranked_titles = list(ranked_df['title'])\n",
    "    ranked_scores = list(ranked_df['score'])\n",
    "    top_ranked_em = n_weighted_embeddings[rankings_index]\n",
    "    ranked_rankings = rankings[rankings_index]\n",
    "    print('about to call kmeans')\n",
    "    results = kMeans(ranked_titles, ranked_scores, ranked_rankings, top_ranked_em, method)\n",
    "\n",
    "#         index = list(ranked_df.itertuples())\n",
    "    print('done with itertuple')\n",
    "    print(results)\n",
    "    indices = [i[1][0] for i in results]\n",
    "    print(indices)\n",
    "    results = [\n",
    "        {\n",
    "            \"type\": \"submission\",\n",
    "            \"title\": ranked_df.iloc[d][\"title\"],\n",
    "            \"subreddit\": ranked_df.iloc[d]['subreddit'],\n",
    "            \"permalink\": ranked_df.iloc[d]['permalink'],\n",
    "            \"score\": ranked_df.iloc[d]['score']\n",
    "        }\n",
    "        for d in [i[1][0] for i in results]\n",
    "    ]\n",
    "    return results, indices, query_weighted, ranked_df, weighted_embeddings[rankings_index]\n",
    "\n",
    "\n",
    "\n",
    "def kMeans(titles, scores, rankings, embeddings, method):\n",
    "    TOP_HITS_KMEANS = max(40,np.sum(scipy.stats.zscore(rankings) > 3.5))\n",
    "    if TOP_HITS_KMEANS > 200:\n",
    "        TOP_HITS_KMEANS = 200\n",
    "    kmeans = KMeans(n_clusters=20, random_state=0).fit(embeddings[:TOP_HITS_KMEANS])\n",
    "\n",
    "    counter = collections.Counter(kmeans.labels_)\n",
    "    most_common = counter.most_common(10)\n",
    "    most_common = set([i[0] for i in most_common])\n",
    "    results = topSimOfEachCluster(kmeans.labels_, 10, most_common)\n",
    "    topScoreOfEachCluster(results, 4, scores)\n",
    "    results = topResultsSorted(results, rankings, scores, method)\n",
    "    return results\n",
    "\n",
    "\n",
    "# cluster number to top num based on similarity\n",
    "def topSimOfEachCluster(cluster_labels, num, most_common):\n",
    "    print('topsimofeachcluster')\n",
    "    res = {}\n",
    "    clusters_included = set(most_common)\n",
    "    for i, el in enumerate(cluster_labels):\n",
    "        if el not in clusters_included:\n",
    "            continue\n",
    "        if el not in res:\n",
    "            res[el] = [i]\n",
    "        elif len(res[el]) < num:\n",
    "            res[el].append(i) \n",
    "    return res \n",
    "\n",
    "#takes topOfEachCluster and gets the top num by score\n",
    "def topScoreOfEachCluster(sim_results, num, scores):\n",
    "    print('topscoreofeachcluster')\n",
    "    for key in sim_results:\n",
    "        sim_results[key].sort(key=lambda x: scores[x], reverse = True)\n",
    "        sim_results[key] = sim_results[key][:num]\n",
    "\n",
    "#sort results by method        \n",
    "def topResultsSorted(results, rankings, scores, method = 'similarity'):\n",
    "    print('topresultssorted')\n",
    "    if method == 'similarity':\n",
    "        for key in results:\n",
    "            results[key].sort(key=lambda x: rankings[x], reverse = True) #sorts within a cluster\n",
    "            sorted_results = sorted(results.items(), key=lambda x: rankings[x[1][0]], reverse = True) #sorts all clusters\n",
    "    elif method == 'score':\n",
    "        for key in results:\n",
    "            results[key].sort(key=lambda x: scores[x], reverse = True)\n",
    "            sorted_results = sorted(results.items(), key=lambda x: scores[x[1][0]], reverse = True)\n",
    "    return sorted_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rocchio(result_indices, select_index, query_weighted, ranked_df, weighted_embeddings):\n",
    "    alpha = 0.2\n",
    "    beta = 0.7\n",
    "    gamma = 0.1\n",
    "    print(weighted_embeddings[select_index].shape)\n",
    "    print(query_weighted.shape)\n",
    "    new_em = alpha * query_weighted + beta*weighted_embeddings[result_indices[select_index]]\n",
    "    irrel_ems = np.zeros(len(query_weighted))\n",
    "    for i in result_indices:\n",
    "        if i == result_indices[select_index]:\n",
    "            continue\n",
    "        irrel_ems += weighted_embeddings[i]\n",
    "    new_em -+ gamma/(len(result_indices) - 1) * irrel_ems\n",
    "    return new_em"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "about to call kmeans\n",
      "topsimofeachcluster\n",
      "topscoreofeachcluster\n",
      "topresultssorted\n",
      "done with itertuple\n",
      "[(8, [0, 3]), (1, [4, 9, 34, 39]), (11, [5, 18]), (5, [6, 8, 30]), (3, [11, 23]), (17, [12, 13, 20]), (9, [15, 24, 35, 44]), (4, [16, 19, 28, 42]), (2, [21, 27, 31, 37]), (15, [22, 46])]\n",
      "[0, 4, 5, 6, 11, 12, 15, 16, 21, 22]\n"
     ]
    }
   ],
   "source": [
    "results, indices, query_weighted, ranked_df, ranked_weighted_embeddings = search(\"information\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'submission',\n",
       "  'title': 'TIL if you put a UPS tracking number in the search, it will give you the link to your tracking information.',\n",
       "  'subreddit': 'til',\n",
       "  'permalink': '/r/til/comments/f8rax/til_if_you_put_a_ups_tracking_number_in_the/',\n",
       "  'score': 1.0},\n",
       " {'type': 'submission',\n",
       "  'title': 'TIL Google recognizes USPS routing numbers + gives you a direct link to tracking info',\n",
       "  'subreddit': 'todayilearned',\n",
       "  'permalink': '/r/todayilearned/comments/c5xsi/til_google_recognizes_usps_routing_numbers_gives/',\n",
       "  'score': 31.0},\n",
       " {'type': 'submission',\n",
       "  'title': 'TIL that there are over 300 companies tracking my every click online and saving my information. I also just found out how to Opt-out.',\n",
       "  'subreddit': 'technology',\n",
       "  'permalink': '/r/technology/comments/p0vge/til_that_there_are_over_300_companies_tracking_my/',\n",
       "  'score': 0.0},\n",
       " {'type': 'submission',\n",
       "  'title': 'TIL it is possible to search for information on Google in a specific date range',\n",
       "  'subreddit': 'todayilearned',\n",
       "  'permalink': '/r/todayilearned/comments/231ml7/til_it_is_possible_to_search_for_information_on/',\n",
       "  'score': 0.0},\n",
       " {'type': 'submission',\n",
       "  'title': 'TIL if you type in your shipping code into google, it directs you straight to the tracking info. ',\n",
       "  'subreddit': 'todayilearned',\n",
       "  'permalink': '/r/todayilearned/comments/je35q/til_if_you_type_in_your_shipping_code_into_google/',\n",
       "  'score': 0.0},\n",
       " {'type': 'submission',\n",
       "  'title': 'TIL that Google gives you actual information and not just links',\n",
       "  'subreddit': 'todayilearned',\n",
       "  'permalink': '/r/todayilearned/comments/fx20t/til_that_google_gives_you_actual_information_and/',\n",
       "  'score': 0.0},\n",
       " {'type': 'submission',\n",
       "  'title': 'Reddit TIL,Facebook is selling your data to advertisers as per the link.I have lots of personal information on Facebook and I assume that even you have put up personal information on your page.Does this make us vulnerable to threats of some form???',\n",
       "  'subreddit': 'todayilearned',\n",
       "  'permalink': '/r/todayilearned/comments/gskjs/reddit_tilfacebook_is_selling_your_data_to/',\n",
       "  'score': 1.0},\n",
       " {'type': 'submission',\n",
       "  'title': \"TIL that if you're using chrome and you type some addresses into the address bar and hit space, it automatically gives you a direct search to that webpage and the search will take you directly to the best result for that website. \",\n",
       "  'subreddit': 'todayilearned',\n",
       "  'permalink': '/r/todayilearned/comments/j4qa2/til_that_if_youre_using_chrome_and_you_type_some/',\n",
       "  'score': 1.0},\n",
       " {'type': 'submission',\n",
       "  'title': 'TIL Google just e-mails me every once in a while to remind me about which services have access to sensitive information, such as my location. Thanks, Google! ',\n",
       "  'subreddit': 'todayilearned',\n",
       "  'permalink': '/r/todayilearned/comments/j7h5n/til_google_just_emails_me_every_once_in_a_while/',\n",
       "  'score': 38.0},\n",
       " {'type': 'submission',\n",
       "  'title': 'TIL the DOD was to create a database of information on everyone in the US. E-mails, social networks, credit cards, calls, medical records, and more without a warrant. Included was funding for biometric surveillance that could identify and track people using surveillance cameras, and other methods.',\n",
       "  'subreddit': 'todayilearned',\n",
       "  'permalink': '/r/todayilearned/comments/1vvzym/til_the_dod_was_to_create_a_database_of/',\n",
       "  'score': 10.0}]"
      ]
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300,)\n",
      "(300,)\n"
     ]
    }
   ],
   "source": [
    "shifted_query_emb = rocchio(indices, 9, query_weighted, ranked_df, ranked_weighted_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "about to call kmeans\n",
      "topsimofeachcluster\n",
      "topscoreofeachcluster\n",
      "topresultssorted\n",
      "done with itertuple\n",
      "[(0, [0, 1, 3]), (6, [2, 4, 5, 28]), (13, [6, 16]), (4, [7, 26]), (11, [8]), (8, [11, 13, 15]), (16, [12, 19]), (12, [17, 23, 30, 33]), (1, [20, 21, 22, 31]), (10, [25, 27])]\n",
      "[0, 2, 6, 7, 8, 11, 12, 17, 20, 25]\n"
     ]
    }
   ],
   "source": [
    "results, indices, query_weighted, ranked_df, ranked_weighted_embeddings = search(shifted_query_emb, rocchio = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
