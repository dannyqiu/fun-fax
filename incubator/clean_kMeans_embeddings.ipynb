{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_lg', disable=[\"parser\", \"tagger\", \"ner\"])\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import scipy\n",
    "import collections\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "EPS = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from . import FUN_FACT_CSV, REQUIRED_COLUMNS\n",
    "TIL_TITLE_CSV = '../data/til_title.csv'\n",
    "REQUIRED_COLUMNS= [\"title\", \"score\", \"permalink\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class KMeans:\n",
    "#     def __init__(self):\n",
    "#         #fun_fact_title_data = pd.read_csv(FUN_FACT_TITLE_CSV).dropna(subset=REQUIRED_COLUMNS)\n",
    "#         self.til_title_data = pd.read_csv(TIL_TITLE_CSV).dropna(subset=REQUIRED_COLUMNS)\n",
    "#         #ysk_title_data = pd.read_csv(YSK_TITLE_CSV).dropna(subset=REQUIRED_COLUMNS)\n",
    "        \n",
    "#         self.data = self.dataframe()\n",
    "#         self.vectorizer = TfidfVectorizer(stop_words='english', max_df=.8, ngram_range=(1,1), dtype=np.float32)\n",
    "#         self.titles = self.data['title']\n",
    "#         self.scores = self.data['score']\n",
    "        \n",
    "#         features = self.vectorizer.get_feature_names()\n",
    "#         self.f_vectors = np.array([nlp.vocab[f].vector for f in self.features])\n",
    "#         embeddings = self.weighted_embeddings(self.vectorizer, self.titles)\n",
    "#         self.n_weighted_embedding = self.embeddings / (np.linalg.norm(self.embeddings, axis=1)[:, np.newaxis] + EPS)\n",
    "        \n",
    "        \n",
    "#     def dataframe(self):\n",
    "#         required_columns = ['title', 'subreddit', 'permalink']\n",
    "#         data = self.til_title_data.dropna(axis='rows', subset=required_columns)\n",
    "#         banned_subreddits = ['circlejerk', 'ShittyTodayILearned', 'TheOnion']\n",
    "#         data = data[~data['subreddit'].isin(banned_subreddits)]\n",
    "#         data = data.reset_index(drop=True)\n",
    "#         return data\n",
    "    \n",
    "#     def weighted_embeddings(self):\n",
    "#         fun_fact_tfidf = self.vectorizer.fit_transform(self.titles)\n",
    "#         weighted_embedding = fun_fact_tfidf.dot(self.f_vectors)\n",
    "#         return weighted_embedding\n",
    "\n",
    "#     def search(self, query, top = 10):\n",
    "#         query_tfidf = self.vectorizer.transform([query])\n",
    "#         query_weighted = query_tfidf.dot(f_vectors).flatten()\n",
    "#         n_query_weighted = query_weighted / np.linalg.norm(query_weighted)\n",
    "#         rankings = n_weighted_embedding.dot(n_query_weighted)\n",
    "#         rankings_index = np.argsort(-rankings)\n",
    "#         fun_fact_df[[\"subreddit\", \"title\", \"score\"]].loc[rankings_index]\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedEmbeddingSearch:\n",
    "\n",
    "    def __init__(self):\n",
    "        print(\"Loading data csv\")\n",
    "        #fun_fact_title_data = pd.read_csv(FUN_FACT_TITLE_CSV).dropna(subset=REQUIRED_COLUMNS)\n",
    "        til_title_data = pd.read_csv(TIL_TITLE_CSV).dropna(subset=REQUIRED_COLUMNS)\n",
    "        #ysk_title_data = pd.read_csv(YSK_TITLE_CSV).dropna(subset=REQUIRED_COLUMNS)\n",
    "\n",
    "        self.title_data = pd.concat([\n",
    "            #fun_fact_title_data,\n",
    "            til_title_data,\n",
    "            #ysk_title_data,\n",
    "        ], join='inner').reset_index(drop=True)\n",
    "\n",
    "        print(\"Computing tf-idf matrix\")\n",
    "        self.vectorizer = TfidfVectorizer(stop_words='english', dtype=np.float32)\n",
    "        tfidf_matrix = self.vectorizer.fit_transform(self.title_data[\"title\"])\n",
    "\n",
    "        print(\"Loading spacy\")\n",
    "        self.nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "        print(\"Computing weighted embeddings\")\n",
    "        features = self.vectorizer.get_feature_names()\n",
    "        self.f_vectors = np.array([self.nlp.vocab[f].vector for f in features])\n",
    "        weighted_embeddings = tfidf_matrix.dot(self.f_vectors)\n",
    "        assert weighted_embeddings.shape == (len(self.title_data.index), 300)\n",
    "        self.n_weighted_embeddings = weighted_embeddings / (np.linalg.norm(weighted_embeddings, axis=1)[:, np.newaxis] + EPS)\n",
    "\n",
    "        #print(\"Compressing pandas dataframe into index\")\n",
    "        #self.index = list(title_data.itertuples())\n",
    "\n",
    "        print(\"Done loading {} rows\".format(len(self.title_data.index)))\n",
    "\n",
    "    def search(self, query, method = 'similarity', top=10):\n",
    "        query_tfidf = self.vectorizer.transform([query])\n",
    "        if query_tfidf.count_nonzero() > 0:\n",
    "            query_weighted = query_tfidf.dot(self.f_vectors).flatten()\n",
    "        # average word embeddings if query words don't exist in our corpus (tfidf matrix)\n",
    "        else:\n",
    "            tokens = self.vectorizer.build_analyzer()(query)\n",
    "            # query was all stopwords, so we'll have to manually tokenize\n",
    "            if not tokens:\n",
    "                tokens = query.lower().split()\n",
    "            query_weighted = np.average([self.nlp.vocab[t].vector for t in tokens], axis=0).flatten()\n",
    "\n",
    "        # if we have no embeddings for the given query, we're out of luck\n",
    "        if np.count_nonzero(query_weighted) == 0:\n",
    "            return []\n",
    "\n",
    "        n_query_weighted = query_weighted / (np.linalg.norm(query_weighted) + EPS)\n",
    "        rankings = self.n_weighted_embeddings.dot(n_query_weighted)\n",
    "        rankings_index = np.argsort(-rankings)\n",
    "        ranked_df = self.title_data.loc[rankings_index]\n",
    "        ranked_titles = list(ranked_df['title'])\n",
    "        ranked_scores = list(ranked_df['score'])\n",
    "        top_ranked_em = self.n_weighted_embeddings[rankings_index]\n",
    "        ranked_rankings = rankings[rankings_index]\n",
    "        print('about to call kmeans')\n",
    "        results = self.kMeans(ranked_titles, ranked_scores, ranked_rankings, top_ranked_em, method)\n",
    "        \n",
    "#         index = list(ranked_df.itertuples())\n",
    "        print('done with itertuple')\n",
    "        results = [\n",
    "            {\n",
    "                \"type\": \"submission\",\n",
    "                \"title\": ranked_df.iloc[d][\"title\"],\n",
    "                \"subreddit\": ranked_df.iloc[d]['subreddit'],\n",
    "                \"permalink\": ranked_df.iloc[d]['permalink'],\n",
    "                \"score\": ranked_df.iloc[d]['score']\n",
    "            }\n",
    "            for d in [i[1][0] for i in results]\n",
    "        ]\n",
    "        return results\n",
    "\n",
    "    \n",
    "    def kMeans(self, titles, scores, rankings, embeddings, method):\n",
    "        TOP_HITS_KMEANS = max(40,np.sum(scipy.stats.zscore(rankings) > 3.5))\n",
    "        if TOP_HITS_KMEANS > 200:\n",
    "            TOP_HITS_KMEANS = 200\n",
    "        kmeans = KMeans(n_clusters=20, random_state=0).fit(embeddings[:TOP_HITS_KMEANS])\n",
    "        \n",
    "        counter = collections.Counter(kmeans.labels_)\n",
    "        most_common = counter.most_common(10)\n",
    "        most_common = set([i[0] for i in most_common])\n",
    "        results = self.topSimOfEachCluster(kmeans.labels_, 10, most_common)\n",
    "        self.topScoreOfEachCluster(results, 4, scores)\n",
    "        results = self.topResultsSorted(results, rankings, scores, method)\n",
    "        return results\n",
    "        \n",
    "        \n",
    "    # cluster number to top num based on similarity\n",
    "    def topSimOfEachCluster(self, cluster_labels, num, most_common):\n",
    "        print('topsimofeachcluster')\n",
    "        res = {}\n",
    "        clusters_included = set(most_common)\n",
    "        for i, el in enumerate(cluster_labels):\n",
    "            if el not in clusters_included:\n",
    "                continue\n",
    "            if el not in res:\n",
    "                res[el] = [i]\n",
    "            elif len(res[el]) < num:\n",
    "                res[el].append(i) \n",
    "        return res \n",
    "    \n",
    "    #takes topOfEachCluster and gets the top num by score\n",
    "    def topScoreOfEachCluster(self, sim_results, num, scores):\n",
    "        print('topscoreofeachcluster')\n",
    "        for key in sim_results:\n",
    "            sim_results[key].sort(key=lambda x: scores[x], reverse = True)\n",
    "            sim_results[key] = sim_results[key][:num]\n",
    "            \n",
    "    #sort results by method        \n",
    "    def topResultsSorted(self, results, rankings, scores, method = 'similarity'):\n",
    "        print('topresultssorted')\n",
    "        if method == 'similarity':\n",
    "            for key in results:\n",
    "                results[key].sort(key=lambda x: rankings[x], reverse = True) #sorts within a cluster\n",
    "                sorted_results = sorted(results.items(), key=lambda x: rankings[x[1][0]], reverse = True) #sorts all clusters\n",
    "        elif method == 'score':\n",
    "            for key in results:\n",
    "                results[key].sort(key=lambda x: scores[x], reverse = True)\n",
    "                sorted_results = sorted(results.items(), key=lambda x: scores[x[1][0]], reverse = True)\n",
    "        return sorted_results\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3214: DtypeWarning: Columns (0,1,2,3,5,8,9,13,14,15,16,19,21,22,24,27,28,33,34,35,41,46,58,60,62,63,64,65,68,74,79,85,92,121) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if (yield from self.run_code(code, result)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing tf-idf matrix\n",
      "Loading spacy\n",
      "Computing weighted embeddings\n",
      "Done loading 324996 rows\n"
     ]
    }
   ],
   "source": [
    "w = WeightedEmbeddingSearch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "about to call kmeans\n",
      "topsimofeachcluster\n",
      "topscoreofeachcluster\n",
      "topresultssorted\n",
      "done with itertuple\n"
     ]
    }
   ],
   "source": [
    "result = w.search(\"oreo cake\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'submission',\n",
       "  'title': 'TIL that Oreo cookies are vegan (at least in the US) ',\n",
       "  'subreddit': 'todayilearned',\n",
       "  'permalink': '/r/todayilearned/comments/1axk4r/til_that_oreo_cookies_are_vegan_at_least_in_the_us/',\n",
       "  'score': 1.0},\n",
       " {'type': 'submission',\n",
       "  'title': 'TIL: The Oreo Secret',\n",
       "  'subreddit': 'todayilearned',\n",
       "  'permalink': '/r/todayilearned/comments/df7iu/til_the_oreo_secret/',\n",
       "  'score': 2.0},\n",
       " {'type': 'submission',\n",
       "  'title': 'TIL: That Oreo cookies in China are not as sweet and have different shape.',\n",
       "  'subreddit': 'todayilearned',\n",
       "  'permalink': '/r/todayilearned/comments/jovoz/til_that_oreo_cookies_in_china_are_not_as_sweet/',\n",
       "  'score': 16.0},\n",
       " {'type': 'submission',\n",
       "  'title': 'TIL Nutella is less nutritious than cake frosting',\n",
       "  'subreddit': 'todayilearned',\n",
       "  'permalink': '/r/todayilearned/comments/180tev/til_nutella_is_less_nutritious_than_cake_frosting/',\n",
       "  'score': 1555.0},\n",
       " {'type': 'submission',\n",
       "  'title': 'TIL about Chinese peanut butter Oreos... They were delicious!',\n",
       "  'subreddit': 'til',\n",
       "  'permalink': '/r/til/comments/g9u3r/til_about_chinese_peanut_butter_oreos_they_were/',\n",
       "  'score': 3.0},\n",
       " {'type': 'submission',\n",
       "  'title': 'TIL Despite its name, the Boston Creme Pie is actually a cake',\n",
       "  'subreddit': 'WTF',\n",
       "  'permalink': '/r/WTF/comments/17q216/til_despite_its_name_the_boston_creme_pie_is/',\n",
       "  'score': 1.0},\n",
       " {'type': 'submission',\n",
       "  'title': 'TIL cupcakes where once called number cakes.',\n",
       "  'subreddit': 'todayilearned',\n",
       "  'permalink': '/r/todayilearned/comments/c2npe/til_cupcakes_where_once_called_number_cakes/',\n",
       "  'score': 2.0},\n",
       " {'type': 'submission',\n",
       "  'title': 'TIL that Ween\\'s \"Chocolate and Cheese\" was dedicated to John Candy',\n",
       "  'subreddit': 'todayilearned',\n",
       "  'permalink': '/r/todayilearned/comments/1aphbd/til_that_weens_chocolate_and_cheese_was_dedicated/',\n",
       "  'score': 16.0},\n",
       " {'type': 'submission',\n",
       "  'title': 'TIL How to make Ice Cream in a bowl made out of goddamn chocolate :3',\n",
       "  'subreddit': 'todayilearned',\n",
       "  'permalink': '/r/todayilearned/comments/jpzlx/til_how_to_make_ice_cream_in_a_bowl_made_out_of/',\n",
       "  'score': 1.0},\n",
       " {'type': 'submission',\n",
       "  'title': 'TIL that raisins are used when making Little Debbie Oatmeal Cream Pies',\n",
       "  'subreddit': 'todayilearned',\n",
       "  'permalink': '/r/todayilearned/comments/hmnmn/til_that_raisins_are_used_when_making_little/',\n",
       "  'score': 3.0}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3214: DtypeWarning: Columns (0,1,2,3,5,8,9,13,14,15,16,19,21,22,24,27,28,33,34,35,41,46,58,60,62,63,64,65,68,74,79,85,92,121) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if (yield from self.run_code(code, result)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing tf-idf matrix\n",
      "Loading spacy\n",
      "Computing weighted embeddings\n",
      "Done loading 324996 rows\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_lg', disable=[\"parser\", \"tagger\", \"ner\"])\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import scipy\n",
    "import collections\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "EPS = 1e-6\n",
    "\n",
    "\n",
    "# from . import FUN_FACT_CSV, REQUIRED_COLUMNS\n",
    "TIL_TITLE_CSV = '../data/til_title.csv'\n",
    "REQUIRED_COLUMNS= [\"title\", \"score\", \"permalink\"]\n",
    "\n",
    "\n",
    "class WeightedEmbeddingSearch:\n",
    "\n",
    "    def __init__(self):\n",
    "        print(\"Loading data csv\")\n",
    "        #fun_fact_title_data = pd.read_csv(FUN_FACT_TITLE_CSV).dropna(subset=REQUIRED_COLUMNS)\n",
    "        til_title_data = pd.read_csv(TIL_TITLE_CSV).dropna(subset=REQUIRED_COLUMNS)\n",
    "        #ysk_title_data = pd.read_csv(YSK_TITLE_CSV).dropna(subset=REQUIRED_COLUMNS)\n",
    "\n",
    "        self.title_data = pd.concat([\n",
    "            #fun_fact_title_data,\n",
    "            til_title_data,\n",
    "            #ysk_title_data,\n",
    "        ], join='inner').reset_index(drop=True)\n",
    "\n",
    "        print(\"Computing tf-idf matrix\")\n",
    "        self.vectorizer = TfidfVectorizer(stop_words='english', dtype=np.float32)\n",
    "        tfidf_matrix = self.vectorizer.fit_transform(self.title_data[\"title\"])\n",
    "\n",
    "        print(\"Loading spacy\")\n",
    "        self.nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "        print(\"Computing weighted embeddings\")\n",
    "        features = self.vectorizer.get_feature_names()\n",
    "        self.f_vectors = np.array([self.nlp.vocab[f].vector for f in features])\n",
    "        weighted_embeddings = tfidf_matrix.dot(self.f_vectors)\n",
    "        assert weighted_embeddings.shape == (len(self.title_data.index), 300)\n",
    "        self.n_weighted_embeddings = weighted_embeddings / (np.linalg.norm(weighted_embeddings, axis=1)[:, np.newaxis] + EPS)\n",
    "\n",
    "        #print(\"Compressing pandas dataframe into index\")\n",
    "        #self.index = list(title_data.itertuples())\n",
    "\n",
    "        print(\"Done loading {} rows\".format(len(self.title_data.index)))\n",
    "\n",
    "    def search(self, query, method = 'similarity', top=10):\n",
    "        query_tfidf = self.vectorizer.transform([query])\n",
    "        if query_tfidf.count_nonzero() > 0:\n",
    "            query_weighted = query_tfidf.dot(self.f_vectors).flatten()\n",
    "        # average word embeddings if query words don't exist in our corpus (tfidf matrix)\n",
    "        else:\n",
    "            tokens = self.vectorizer.build_analyzer()(query)\n",
    "            # query was all stopwords, so we'll have to manually tokenize\n",
    "            if not tokens:\n",
    "                tokens = query.lower().split()\n",
    "            query_weighted = np.average([self.nlp.vocab[t].vector for t in tokens], axis=0).flatten()\n",
    "\n",
    "        # if we have no embeddings for the given query, we're out of luck\n",
    "        if np.count_nonzero(query_weighted) == 0:\n",
    "            return []\n",
    "\n",
    "        n_query_weighted = query_weighted / (np.linalg.norm(query_weighted) + EPS)\n",
    "        rankings = self.n_weighted_embeddings.dot(n_query_weighted)\n",
    "        rankings_index = np.argsort(-rankings)\n",
    "        ranked_df = self.title_data.loc[rankings_index]\n",
    "        ranked_titles = list(ranked_df['title'])\n",
    "        ranked_scores = list(ranked_df['score'])\n",
    "        top_ranked_em = self.n_weighted_embeddings[rankings_index]\n",
    "        ranked_rankings = rankings[rankings_index]\n",
    "        results = self.kMeans(ranked_titles, ranked_scores, ranked_rankings, top_ranked_em, method)\n",
    "        \n",
    "#         index = list(ranked_df.itertuples())\n",
    "        results = [\n",
    "            {\n",
    "                \"type\": \"submission\",\n",
    "                \"title\": ranked_df.iloc[d][\"title\"],\n",
    "                \"subreddit\": ranked_df.iloc[d]['subreddit'],\n",
    "                \"permalink\": ranked_df.iloc[d]['permalink'],\n",
    "                \"score\": ranked_df.iloc[d]['score']\n",
    "            }\n",
    "            for d in [i[1][0] for i in results]\n",
    "        ]\n",
    "        return results\n",
    "\n",
    "    \n",
    "    def kMeans(self, titles, scores, rankings, embeddings, method):\n",
    "        TOP_HITS_KMEANS = max(40,np.sum(scipy.stats.zscore(rankings) > 3.5))\n",
    "        if TOP_HITS_KMEANS > 200:\n",
    "            TOP_HITS_KMEANS = 200\n",
    "        kmeans = KMeans(n_clusters=20, random_state=0).fit(embeddings[:TOP_HITS_KMEANS])\n",
    "        \n",
    "        counter = collections.Counter(kmeans.labels_)\n",
    "        most_common = counter.most_common(10)\n",
    "        most_common = set([i[0] for i in most_common])\n",
    "        results = self.topSimOfEachCluster(kmeans.labels_, 10, most_common)\n",
    "        self.topScoreOfEachCluster(results, 4, scores)\n",
    "        results = self.topResultsSorted(results, rankings, scores, method)\n",
    "        return results\n",
    "        \n",
    "        \n",
    "    # cluster number to top num based on similarity\n",
    "    def topSimOfEachCluster(self, cluster_labels, num, most_common):\n",
    "        res = {}\n",
    "        clusters_included = set(most_common)\n",
    "        for i, el in enumerate(cluster_labels):\n",
    "            if el not in clusters_included:\n",
    "                continue\n",
    "            if el not in res:\n",
    "                res[el] = [i]\n",
    "            elif len(res[el]) < num:\n",
    "                res[el].append(i) \n",
    "        return res \n",
    "    \n",
    "    #takes topOfEachCluster and gets the top num by score\n",
    "    def topScoreOfEachCluster(self, sim_results, num, scores):\n",
    "        for key in sim_results:\n",
    "            sim_results[key].sort(key=lambda x: scores[x], reverse = True)\n",
    "            sim_results[key] = sim_results[key][:num]\n",
    "            \n",
    "    #sort results by method        \n",
    "    def topResultsSorted(self, results, rankings, scores, method = 'similarity'):\n",
    "        if method == 'similarity':\n",
    "            for key in results:\n",
    "                results[key].sort(key=lambda x: rankings[x], reverse = True) #sorts within a cluster\n",
    "                sorted_results = sorted(results.items(), key=lambda x: rankings[x[1][0]], reverse = True) #sorts all clusters\n",
    "        elif method == 'score':\n",
    "            for key in results:\n",
    "                results[key].sort(key=lambda x: scores[x], reverse = True)\n",
    "                sorted_results = sorted(results.items(), key=lambda x: scores[x[1][0]], reverse = True)\n",
    "        return sorted_results\n",
    "        \n",
    "w = WeightedEmbeddingSearch()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = w.search(\"oreo cake\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'submission',\n",
       "  'title': 'TIL that Oreo cookies are vegan (at least in the US) ',\n",
       "  'subreddit': 'todayilearned',\n",
       "  'permalink': '/r/todayilearned/comments/1axk4r/til_that_oreo_cookies_are_vegan_at_least_in_the_us/',\n",
       "  'score': 1.0},\n",
       " {'type': 'submission',\n",
       "  'title': 'TIL: The Oreo Secret',\n",
       "  'subreddit': 'todayilearned',\n",
       "  'permalink': '/r/todayilearned/comments/df7iu/til_the_oreo_secret/',\n",
       "  'score': 2.0},\n",
       " {'type': 'submission',\n",
       "  'title': 'TIL: That Oreo cookies in China are not as sweet and have different shape.',\n",
       "  'subreddit': 'todayilearned',\n",
       "  'permalink': '/r/todayilearned/comments/jovoz/til_that_oreo_cookies_in_china_are_not_as_sweet/',\n",
       "  'score': 16.0},\n",
       " {'type': 'submission',\n",
       "  'title': 'TIL Nutella is less nutritious than cake frosting',\n",
       "  'subreddit': 'todayilearned',\n",
       "  'permalink': '/r/todayilearned/comments/180tev/til_nutella_is_less_nutritious_than_cake_frosting/',\n",
       "  'score': 1555.0},\n",
       " {'type': 'submission',\n",
       "  'title': 'TIL about Chinese peanut butter Oreos... They were delicious!',\n",
       "  'subreddit': 'til',\n",
       "  'permalink': '/r/til/comments/g9u3r/til_about_chinese_peanut_butter_oreos_they_were/',\n",
       "  'score': 3.0},\n",
       " {'type': 'submission',\n",
       "  'title': 'TIL Despite its name, the Boston Creme Pie is actually a cake',\n",
       "  'subreddit': 'WTF',\n",
       "  'permalink': '/r/WTF/comments/17q216/til_despite_its_name_the_boston_creme_pie_is/',\n",
       "  'score': 1.0},\n",
       " {'type': 'submission',\n",
       "  'title': 'TIL cupcakes where once called number cakes.',\n",
       "  'subreddit': 'todayilearned',\n",
       "  'permalink': '/r/todayilearned/comments/c2npe/til_cupcakes_where_once_called_number_cakes/',\n",
       "  'score': 2.0},\n",
       " {'type': 'submission',\n",
       "  'title': 'TIL that Ween\\'s \"Chocolate and Cheese\" was dedicated to John Candy',\n",
       "  'subreddit': 'todayilearned',\n",
       "  'permalink': '/r/todayilearned/comments/1aphbd/til_that_weens_chocolate_and_cheese_was_dedicated/',\n",
       "  'score': 16.0},\n",
       " {'type': 'submission',\n",
       "  'title': 'TIL How to make Ice Cream in a bowl made out of goddamn chocolate :3',\n",
       "  'subreddit': 'todayilearned',\n",
       "  'permalink': '/r/todayilearned/comments/jpzlx/til_how_to_make_ice_cream_in_a_bowl_made_out_of/',\n",
       "  'score': 1.0},\n",
       " {'type': 'submission',\n",
       "  'title': 'TIL that raisins are used when making Little Debbie Oatmeal Cream Pies',\n",
       "  'subreddit': 'todayilearned',\n",
       "  'permalink': '/r/todayilearned/comments/hmnmn/til_that_raisins_are_used_when_making_little/',\n",
       "  'score': 3.0}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
