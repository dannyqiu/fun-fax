{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_lg', disable=[\"parser\", \"tagger\", \"ner\"])\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import scipy\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "EPS = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from . import FUN_FACT_CSV, REQUIRED_COLUMNS\n",
    "TIL_TITLE_CSV = '../data/til_title.csv'\n",
    "REQUIRED_COLUMNS= [\"title\", \"score\", \"permalink\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMeans:\n",
    "    def __init__(self):\n",
    "        #fun_fact_title_data = pd.read_csv(FUN_FACT_TITLE_CSV).dropna(subset=REQUIRED_COLUMNS)\n",
    "        self.til_title_data = pd.read_csv(TIL_TITLE_CSV).dropna(subset=REQUIRED_COLUMNS)\n",
    "        #ysk_title_data = pd.read_csv(YSK_TITLE_CSV).dropna(subset=REQUIRED_COLUMNS)\n",
    "        \n",
    "        self.data = self.dataframe()\n",
    "        self.vectorizer = TfidfVectorizer(stop_words='english', max_df=.8, ngram_range=(1,1), dtype=np.float32)\n",
    "        self.titles = self.data['title']\n",
    "        self.scores = self.data['score']\n",
    "        \n",
    "        features = self.vectorizer.get_feature_names()\n",
    "        self.f_vectors = np.array([nlp.vocab[f].vector for f in self.features])\n",
    "        embeddings = self.weighted_embeddings(self.vectorizer, self.titles)\n",
    "        self.n_weighted_embedding = self.embeddings / (np.linalg.norm(self.embeddings, axis=1)[:, np.newaxis] + EPS)\n",
    "        \n",
    "        \n",
    "    def dataframe(self):\n",
    "        required_columns = ['title', 'subreddit', 'permalink']\n",
    "        data = self.til_title_data.dropna(axis='rows', subset=required_columns)\n",
    "        banned_subreddits = ['circlejerk', 'ShittyTodayILearned', 'TheOnion']\n",
    "        data = data[~data['subreddit'].isin(banned_subreddits)]\n",
    "        data = data.reset_index(drop=True)\n",
    "        return data\n",
    "    \n",
    "    def weighted_embeddings(self):\n",
    "        fun_fact_tfidf = self.vectorizer.fit_transform(self.titles)\n",
    "        weighted_embedding = fun_fact_tfidf.dot(self.f_vectors)\n",
    "        return weighted_embedding\n",
    "\n",
    "    def search(self, query, top = 10):\n",
    "        query_tfidf = self.vectorizer.transform([query])\n",
    "        query_weighted = query_tfidf.dot(f_vectors).flatten()\n",
    "        n_query_weighted = query_weighted / np.linalg.norm(query_weighted)\n",
    "        rankings = n_weighted_embedding.dot(n_query_weighted)\n",
    "        rankings_index = np.argsort(-rankings)\n",
    "        fun_fact_df[[\"subreddit\", \"title\", \"score\"]].loc[rankings_index]\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedEmbeddingSearch:\n",
    "\n",
    "    def __init__(self):\n",
    "        print(\"Loading data csv\")\n",
    "        #fun_fact_title_data = pd.read_csv(FUN_FACT_TITLE_CSV).dropna(subset=REQUIRED_COLUMNS)\n",
    "        til_title_data = pd.read_csv(TIL_TITLE_CSV).dropna(subset=REQUIRED_COLUMNS)\n",
    "        #ysk_title_data = pd.read_csv(YSK_TITLE_CSV).dropna(subset=REQUIRED_COLUMNS)\n",
    "\n",
    "        title_data = pd.concat([\n",
    "            #fun_fact_title_data,\n",
    "            til_title_data,\n",
    "            #ysk_title_data,\n",
    "        ], join='inner').reset_index(drop=True)\n",
    "\n",
    "        print(\"Computing tf-idf matrix\")\n",
    "        self.vectorizer = TfidfVectorizer(stop_words='english', dtype=np.float32)\n",
    "        tfidf_matrix = self.vectorizer.fit_transform(title_data[\"title\"])\n",
    "\n",
    "        print(\"Loading spacy\")\n",
    "        self.nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "        print(\"Computing weighted embeddings\")\n",
    "        features = self.vectorizer.get_feature_names()\n",
    "        self.f_vectors = np.array([self.nlp.vocab[f].vector for f in features])\n",
    "        weighted_embeddings = tfidf_matrix.dot(self.f_vectors)\n",
    "        assert weighted_embeddings.shape == (len(title_data.index), 300)\n",
    "        self.n_weighted_embeddings = weighted_embeddings / (np.linalg.norm(weighted_embeddings, axis=1)[:, np.newaxis] + EPS)\n",
    "\n",
    "        print(\"Compressing pandas dataframe into index\")\n",
    "        self.index = list(title_data.itertuples())\n",
    "\n",
    "        print(\"Done loading {} rows\".format(len(title_data.index)))\n",
    "\n",
    "    def search(self, query, top=10):\n",
    "        query_tfidf = self.vectorizer.transform([query])\n",
    "        if query_tfidf.count_nonzero() > 0:\n",
    "            query_weighted = query_tfidf.dot(self.f_vectors).flatten()\n",
    "        # average word embeddings if query words don't exist in our corpus (tfidf matrix)\n",
    "        else:\n",
    "            tokens = self.vectorizer.build_analyzer()(query)\n",
    "            # query was all stopwords, so we'll have to manually tokenize\n",
    "            if not tokens:\n",
    "                tokens = query.lower().split()\n",
    "            query_weighted = np.average([self.nlp.vocab[t].vector for t in tokens], axis=0).flatten()\n",
    "\n",
    "        # if we have no embeddings for the given query, we're out of luck\n",
    "        if np.count_nonzero(query_weighted) == 0:\n",
    "            return []\n",
    "\n",
    "        n_query_weighted = query_weighted / (np.linalg.norm(query_weighted) + EPS)\n",
    "        rankings = self.n_weighted_embeddings.dot(n_query_weighted)\n",
    "        rel = np.argsort(-rankings)[:top]\n",
    "        results = [\n",
    "            {\n",
    "                \"type\": \"submission\",\n",
    "                \"title\": self.index[d].title,\n",
    "                \"subreddit\": self.index[d].subreddit,\n",
    "                \"permalink\": self.index[d].permalink,\n",
    "                \"score\": self.index[d].score,\n",
    "            }\n",
    "            for d in rel\n",
    "        ]\n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data csv\n",
      "Computing tf-idf matrix\n",
      "Loading spacy\n",
      "Computing weighted embeddings\n",
      "Compressing pandas dataframe into index\n",
      "Done loading 324996 rows\n"
     ]
    }
   ],
   "source": [
    "w = WeightedEmbeddingSearch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
